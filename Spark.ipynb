{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOoFgHbWgoaZ/vS4zjkJgNy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apaillard/BigData/blob/main/Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLrqV4gsEW1I"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"]=\"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCukmMtVEY4s"
      },
      "source": [
        "dataset = spark.read.csv('BostonHousing.csv',inferSchema=True ,header =True)\n",
        "dataset.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ4TgYjfEg7Y"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "#Input all the features in one vector column\n",
        "assembler = VectorAssembler(inputCols=['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'], outputCol = 'Attributes')\n",
        "output = assembler.transform(dataset)\n",
        "#Input vs Output\n",
        "finalized_data = output.select(\"Attributes\",\"medv\")\n",
        "finalized_data.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L1RfV8wEp76"
      },
      "source": [
        "#Split training and testing data\n",
        "train_data,test_data = finalized_data.randomSplit([0.8,0.2])\n",
        "regressor = LinearRegression(featuresCol = 'Attributes', labelCol = 'medv')\n",
        "#Learn to fit the model from training set\n",
        "regressor = regressor.fit(train_data)\n",
        "#To predict the prices on testing set\n",
        "pred = regressor.evaluate(test_data)\n",
        "#Predict the model\n",
        "pred.predictions.show()\n",
        "#coefficient of the regression model\n",
        "coeff = regressor.coefficients\n",
        "#X and Y intercept\n",
        "intr = regressor.intercept\n",
        "print (\"The coefficient of the model is : %a\" %coeff)\n",
        "print (\"The Intercept of the model is : %f\" %intr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sR3KmAJEsHr"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "eval = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "# Root Mean Square Error\n",
        "rmse = eval.evaluate(pred.predictions)\n",
        "print(\"RMSE: %.3f\" % rmse)\n",
        "# Mean Square Error\n",
        "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
        "print(\"MSE: %.3f\" % mse)\n",
        "# Mean Absolute Error\n",
        "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
        "print(\"MAE: %.3f\" % mae)\n",
        "# r2 - coefficient of determination\n",
        "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})\n",
        "print(\"r2: %.3f\" %r2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4yi2eowEubz"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "# Trains a k-means model.\n",
        "kmeans = KMeans(featuresCol='Attributes').setK(2).setSeed(1)\n",
        "model = kmeans.fit(finalized_data)\n",
        "# Make predictions\n",
        "predictions = model.transform(finalized_data)\n",
        "predictions.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf3M5BsnEwmz"
      },
      "source": [
        "datasetCalls = spark.read.csv('CallsData.csv', header =True)\n",
        "datasetContract = spark.read.csv('ContractData.csv', header =True)\n",
        "datasetCalls.show()\n",
        "datasetContract.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}